1. 어떤 문제를 풀고자 했는가? (Abstract)
단어의 의미를 밀집 벡터 공간에 분산 표현함으로써 단어 간 유사도를 계산하고, 이를 자연어 처리 과제에 활용하고자 하였다.

2. 어떤 동기/상황/문제점에서 이 연구가 시작되었는가? (Introduction)
기존의 one-hot 인코딩 방식은 단어의 유사성을 표현하지 못하는 한계가 있었다. 단어의 의미를 보다 효과적으로 표현하고 유사도를 반영할 수 있는 분산 표현 기법이 필요하였다.

3. 관련된 사전 연구는 어떤 것이 있는가? (Related works)
Word2Vec은 2013년 구글에서 개발한 단어 임베딩 기법으로, CBOW와 Skip-gram 두 가지 모델을 제안하였다. ELMo는 2018년 알렌AI에서 개발한 문맥 의존적 단어 표현 기법이다.  

4. 이 연구의 접근 방법은 무엇인가? (Method)
Word2Vec은 단어 주변 문맥을 활용하여 유사한 단어에 유사한 벡터를 할당하는 방식이다. ELMo는 양방향 LSTM 언어 모델을 통해 문맥을 반영한 동적 단어 표현을 학습한다.

5. 실험은 어떻게 이루어졌는가? (Experiments)  
구체적 실험 방법은 생략되었으나, 각 기법의 성능을 다양한 벤치마크 데이터셋에 적용하여 평가하였을 것이다.

6. 무엇을 알아냈으며, 한계점은 무엇인가? (Discussion)
Word2Vec은 단어 유사도 측정과 단어 유추 관계 파악에 효과적이나 문맥을 고려하지 않는 한계가 있다. ELMo는 문맥 의존적 단어 표현을 학습하여 자연어 이해 과제에 더 적합하다.

7. 결론 및 요약 (Conclusion)
Word2Vec과 ELMo는 모두 단어를 밀집 벡터로 표현하여 단어 유사도 계산과 자연어 처리에 활용된다. 그러나 ELMo가 문맥을 반영하여 Word2Vec에 비해 진화된 방식이라고 할 수 있다.

8. 다른 것과의 차별점은 무엇인가? 
Word2Vec은 정적인 단어 벡터를 학습하지만, ELMo는 문맥에 따라 동적으로 변화하는 단어 표현을 학습한다. 또한 ELMo는 전이학습이 가능하다는 차별점이 있다.
